digraph {
	graph [size="202.2,202.2"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2146705655344 [label="
 (1, 3, 256, 256)" fillcolor=darkolivegreen1]
	2146705779440 [label=ConvolutionBackward0]
	2146705779056 -> 2146705779440
	2146705779056 [label=MulBackward0]
	2146705779872 -> 2146705779056
	2146705779872 [label=PermuteBackward0]
	2146705778768 -> 2146705779872
	2146705778768 [label=AddBackward0]
	2146705778576 -> 2146705778768
	2146705778576 [label=UnsafeViewBackward0]
	2146705778240 -> 2146705778576
	2146705778240 [label=MmBackward0]
	2146705778144 -> 2146705778240
	2146705778144 [label=ReshapeAliasBackward0]
	2146705777904 -> 2146705778144
	2146705777904 [label=ReshapeAliasBackward0]
	2146705777712 -> 2146705777904
	2146705777712 [label=SumBackward1]
	2146705777520 -> 2146705777712
	2146705777520 [label=MulBackward0]
	2146705777328 -> 2146705777520
	2146705777328 [label=UnsqueezeBackward0]
	2146705777088 -> 2146705777328
	2146705777088 [label=SoftmaxBackward0]
	2146705776896 -> 2146705777088
	2146705776896 [label=ReshapeAliasBackward0]
	2146705776704 -> 2146705776896
	2146705776704 [label=MmBackward0]
	2146705777280 -> 2146705776704
	2146705777280 [label=GeluBackward0]
	2146705747632 -> 2146705777280
	2146705747632 [label=MmBackward0]
	2146705747440 -> 2146705747632
	2146705747440 [label=SumBackward1]
	2146705747104 -> 2146705747440
	2146705747104 [label=SumBackward1]
	2146705777376 -> 2146705747104
	2146705777376 [label=ReshapeAliasBackward0]
	2146705746864 -> 2146705777376
	2146705746864 [label=StackBackward0]
	2146705746768 -> 2146705746864
	2146705746768 [label=AsStridedBackward0]
	2146705746384 -> 2146705746768
	2146705746384 [label=CopySlices]
	2146705746192 -> 2146705746384
	2146705746192 [label=CopySlices]
	2146705745952 -> 2146705746192
	2146705745952 [label=CopySlices]
	2146705745712 -> 2146705745952
	2146705745712 [label=CopySlices]
	2146705745376 -> 2146705745712
	2146705745376 [label=CopySlices]
	2146705745136 -> 2146705745376
	2146705745136 [label=CopySlices]
	2146705744896 -> 2146705745136
	2146705744896 [label=CopySlices]
	2146705744656 -> 2146705744896
	2146705744656 [label=CopySlices]
	2146705744320 -> 2146705744656
	2146705744320 [label=AddBackward0]
	2146705744176 -> 2146705744320
	2146705744176 [label=UnsafeViewBackward0]
	2146705743984 -> 2146705744176
	2146705743984 [label=MmBackward0]
	2146705719008 -> 2146705743984
	2146705719008 [label=ReshapeAliasBackward0]
	2146705718768 -> 2146705719008
	2146705718768 [label=PermuteBackward0]
	2146705718672 -> 2146705718768
	2146705718672 [label=ReluBackward0]
	2146705718480 -> 2146705718672
	2146705718480 [label=CudnnBatchNormBackward0]
	2146705718288 -> 2146705718480
	2146705718288 [label=ConvolutionBackward0]
	2146705717904 -> 2146705718288
	2146705717904 [label=ReluBackward0]
	2146705717616 -> 2146705717904
	2146705717616 [label=CudnnBatchNormBackward0]
	2146705717424 -> 2146705717616
	2146705717424 [label=ConvolutionBackward0]
	2146705717040 -> 2146705717424
	2146705717040 [label=CatBackward0]
	2146705716752 -> 2146705717040
	2146705716752 [label=ReluBackward0]
	2146705716416 -> 2146705716752
	2146705716416 [label=CudnnBatchNormBackward0]
	2146705716224 -> 2146705716416
	2146705716224 [label=ConvolutionBackward0]
	2146705715936 -> 2146705716224
	2146705715936 [label=ReluBackward0]
	2146705715552 -> 2146705715936
	2146705715552 [label=CudnnBatchNormBackward0]
	2146705715360 -> 2146705715552
	2146705715360 [label=ConvolutionBackward0]
	2146705715264 -> 2146705715360
	2146687762816 [label="conv.double_conv.0.conv.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2146687762816 -> 2146705715264
	2146705715264 [label=AccumulateGrad]
	2146705678144 -> 2146705715360
	2146687762896 [label="conv.double_conv.0.conv.0.bias
 (64)" fillcolor=lightblue]
	2146687762896 -> 2146705678144
	2146705678144 [label=AccumulateGrad]
	2146705715504 -> 2146705715552
	2146687762976 [label="conv.double_conv.0.conv.1.weight
 (64)" fillcolor=lightblue]
	2146687762976 -> 2146705715504
	2146705715504 [label=AccumulateGrad]
	2146705715744 -> 2146705715552
	2146687763056 [label="conv.double_conv.0.conv.1.bias
 (64)" fillcolor=lightblue]
	2146687763056 -> 2146705715744
	2146705715744 [label=AccumulateGrad]
	2146705716080 -> 2146705716224
	2146687763536 [label="conv.double_conv.1.conv.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2146687763536 -> 2146705716080
	2146705716080 [label=AccumulateGrad]
	2146705716128 -> 2146705716224
	2146687763616 [label="conv.double_conv.1.conv.0.bias
 (64)" fillcolor=lightblue]
	2146687763616 -> 2146705716128
	2146705716128 [label=AccumulateGrad]
	2146705716368 -> 2146705716416
	2146687763696 [label="conv.double_conv.1.conv.1.weight
 (64)" fillcolor=lightblue]
	2146687763696 -> 2146705716368
	2146705716368 [label=AccumulateGrad]
	2146705716608 -> 2146705716416
	2146687763776 [label="conv.double_conv.1.conv.1.bias
 (64)" fillcolor=lightblue]
	2146687763776 -> 2146705716608
	2146705716608 [label=AccumulateGrad]
	2146705716800 -> 2146705717040
	2146705716800 [label=ConstantPadNdBackward0]
	2146705715888 -> 2146705716800
	2146705715888 [label=UpsampleBilinear2DBackward1]
	2146705715312 -> 2146705715888
	2146705715312 [label=ReluBackward0]
	2146705716560 -> 2146705715312
	2146705716560 [label=CudnnBatchNormBackward0]
	2146705677904 -> 2146705716560
	2146705677904 [label=ConvolutionBackward0]
	2146705677520 -> 2146705677904
	2146705677520 [label=MulBackward0]
	2146705677232 -> 2146705677520
	2146705677232 [label=PermuteBackward0]
	2146705676896 -> 2146705677232
	2146705676896 [label=AddBackward0]
	2146705676704 -> 2146705676896
	2146705676704 [label=UnsafeViewBackward0]
	2146705676560 -> 2146705676704
	2146705676560 [label=MmBackward0]
	2146705676368 -> 2146705676560
	2146705676368 [label=ReshapeAliasBackward0]
	2146705676032 -> 2146705676368
	2146705676032 [label=ReshapeAliasBackward0]
	2146705675840 -> 2146705676032
	2146705675840 [label=SumBackward1]
	2146705675744 -> 2146705675840
	2146705675744 [label=MulBackward0]
	2146705675552 -> 2146705675744
	2146705675552 [label=UnsqueezeBackward0]
	2146705675312 -> 2146705675552
	2146705675312 [label=SoftmaxBackward0]
	2146705675120 -> 2146705675312
	2146705675120 [label=ReshapeAliasBackward0]
	2146705674928 -> 2146705675120
	2146705674928 [label=MmBackward0]
	2146705674832 -> 2146705674928
	2146705674832 [label=GeluBackward0]
	2146705674496 -> 2146705674832
	2146705674496 [label=MmBackward0]
	2146705674304 -> 2146705674496
	2146705674304 [label=SumBackward1]
	2146705645328 -> 2146705674304
	2146705645328 [label=SumBackward1]
	2146705675696 -> 2146705645328
	2146705675696 [label=ReshapeAliasBackward0]
	2146705645088 -> 2146705675696
	2146705645088 [label=StackBackward0]
	2146705644896 -> 2146705645088
	2146705644896 [label=AsStridedBackward0]
	2146705644512 -> 2146705644896
	2146705644512 [label=CopySlices]
	2146705644416 -> 2146705644512
	2146705644416 [label=CopySlices]
	2146705644176 -> 2146705644416
	2146705644176 [label=CopySlices]
	2146705643840 -> 2146705644176
	2146705643840 [label=CopySlices]
	2146705643600 -> 2146705643840
	2146705643600 [label=CopySlices]
	2146705643360 -> 2146705643600
	2146705643360 [label=CopySlices]
	2146705643120 -> 2146705643360
	2146705643120 [label=CopySlices]
	2146705642784 -> 2146705643120
	2146705642784 [label=CopySlices]
	2146705642640 -> 2146705642784
	2146705642640 [label=AddBackward0]
	2146705642304 -> 2146705642640
	2146705642304 [label=UnsafeViewBackward0]
	2146705642064 -> 2146705642304
	2146705642064 [label=MmBackward0]
	2146705641872 -> 2146705642064
	2146705641872 [label=ReshapeAliasBackward0]
	2146705641632 -> 2146705641872
	2146705641632 [label=PermuteBackward0]
	2146705641584 -> 2146705641632
	2146705641584 [label=ReluBackward0]
	2146705620704 -> 2146705641584
	2146705620704 [label=CudnnBatchNormBackward0]
	2146705620512 -> 2146705620704
	2146705620512 [label=ConvolutionBackward0]
	2146705620224 -> 2146705620512
	2146705620224 [label=ReluBackward0]
	2146705619840 -> 2146705620224
	2146705619840 [label=CudnnBatchNormBackward0]
	2146705619648 -> 2146705619840
	2146705619648 [label=ConvolutionBackward0]
	2146705619360 -> 2146705619648
	2146705619360 [label=CatBackward0]
	2146705618976 -> 2146705619360
	2146705618976 [label=MaxPool2DWithIndicesBackward0]
	2146705618736 -> 2146705618976
	2146705618736 [label=ReluBackward0]
	2146705618640 -> 2146705618736
	2146705618640 [label=CudnnBatchNormBackward0]
	2146705618448 -> 2146705618640
	2146705618448 [label=ConvolutionBackward0]
	2146705618064 -> 2146705618448
	2146705618064 [label=ReluBackward0]
	2146705617776 -> 2146705618064
	2146705617776 [label=CudnnBatchNormBackward0]
	2146705617584 -> 2146705617776
	2146705617584 [label=ConvolutionBackward0]
	2146705716752 -> 2146705617584
	2146705617200 -> 2146705617584
	2146687764176 [label="down1.double_conv.double_conv.0.conv.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2146687764176 -> 2146705617200
	2146705617200 [label=AccumulateGrad]
	2146705617248 -> 2146705617584
	2146687764256 [label="down1.double_conv.double_conv.0.conv.0.bias
 (128)" fillcolor=lightblue]
	2146687764256 -> 2146705617248
	2146705617248 [label=AccumulateGrad]
	2146705617632 -> 2146705617776
	2146687764336 [label="down1.double_conv.double_conv.0.conv.1.weight
 (128)" fillcolor=lightblue]
	2146687764336 -> 2146705617632
	2146705617632 [label=AccumulateGrad]
	2146705617872 -> 2146705617776
	2146687764416 [label="down1.double_conv.double_conv.0.conv.1.bias
 (128)" fillcolor=lightblue]
	2146687764416 -> 2146705617872
	2146705617872 [label=AccumulateGrad]
	2146705618112 -> 2146705618448
	2146687764816 [label="down1.double_conv.double_conv.1.conv.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2146687764816 -> 2146705618112
	2146705618112 [label=AccumulateGrad]
	2146705618256 -> 2146705618448
	2146687764896 [label="down1.double_conv.double_conv.1.conv.0.bias
 (128)" fillcolor=lightblue]
	2146687764896 -> 2146705618256
	2146705618256 [label=AccumulateGrad]
	2146705618496 -> 2146705618640
	2146687764976 [label="down1.double_conv.double_conv.1.conv.1.weight
 (128)" fillcolor=lightblue]
	2146687764976 -> 2146705618496
	2146705618496 [label=AccumulateGrad]
	2146705618928 -> 2146705618640
	2146687765056 [label="down1.double_conv.double_conv.1.conv.1.bias
 (128)" fillcolor=lightblue]
	2146687765056 -> 2146705618928
	2146705618928 [label=AccumulateGrad]
	2146705619120 -> 2146705619360
	2146705619120 [label=ConstantPadNdBackward0]
	2146705618304 -> 2146705619120
	2146705618304 [label=UpsampleBilinear2DBackward1]
	2146705617824 -> 2146705618304
	2146705617824 [label=ReluBackward0]
	2146705617392 -> 2146705617824
	2146705617392 [label=CudnnBatchNormBackward0]
	2146705617008 -> 2146705617392
	2146705617008 [label=ConvolutionBackward0]
	2146705607936 -> 2146705617008
	2146705607936 [label=MulBackward0]
	2146705606640 -> 2146705607936
	2146705606640 [label=MulBackward0]
	2146705605392 -> 2146705606640
	2146705605392 [label=AddBackward0]
	2146705604672 -> 2146705605392
	2146705604672 [label=AddBackward0]
	2146705604720 -> 2146705604672
	2146705604720 [label=MulBackward0]
	2146688878768 -> 2146705604720
	2146688878768 [label=ReluBackward0]
	2146688878192 -> 2146688878768
	2146688878192 [label=CudnnBatchNormBackward0]
	2146688877568 -> 2146688878192
	2146688877568 [label=ConvolutionBackward0]
	2146688876656 -> 2146688877568
	2146688876656 [label=ReluBackward0]
	2146688613680 -> 2146688876656
	2146688613680 [label=CudnnBatchNormBackward0]
	2146688613968 -> 2146688613680
	2146688613968 [label=ConvolutionBackward0]
	2146688614352 -> 2146688613968
	2146688614352 [label=CatBackward0]
	2146688851632 -> 2146688614352
	2146688851632 [label=MaxPool2DWithIndicesBackward0]
	2146688850864 -> 2146688851632
	2146688850864 [label=ReluBackward0]
	2146688850768 -> 2146688850864
	2146688850768 [label=CudnnBatchNormBackward0]
	2146688850528 -> 2146688850768
	2146688850528 [label=ConvolutionBackward0]
	2146688850144 -> 2146688850528
	2146688850144 [label=ReluBackward0]
	2146688849568 -> 2146688850144
	2146688849568 [label=CudnnBatchNormBackward0]
	2146688849040 -> 2146688849568
	2146688849040 [label=ConvolutionBackward0]
	2146705618976 -> 2146688849040
	2146688847984 -> 2146688849040
	2146687765456 [label="down2.double_conv.double_conv.0.conv.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2146687765456 -> 2146688847984
	2146688847984 [label=AccumulateGrad]
	2146688848272 -> 2146688849040
	2146687765536 [label="down2.double_conv.double_conv.0.conv.0.bias
 (256)" fillcolor=lightblue]
	2146687765536 -> 2146688848272
	2146688848272 [label=AccumulateGrad]
	2146688849616 -> 2146688849568
	2146687765616 [label="down2.double_conv.double_conv.0.conv.1.weight
 (256)" fillcolor=lightblue]
	2146687765616 -> 2146688849616
	2146688849616 [label=AccumulateGrad]
	2146688850000 -> 2146688849568
	2146687765696 [label="down2.double_conv.double_conv.0.conv.1.bias
 (256)" fillcolor=lightblue]
	2146687765696 -> 2146688850000
	2146688850000 [label=AccumulateGrad]
	2146688850240 -> 2146688850528
	2146687766096 [label="down2.double_conv.double_conv.1.conv.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2146687766096 -> 2146688850240
	2146688850240 [label=AccumulateGrad]
	2146688850288 -> 2146688850528
	2146687766176 [label="down2.double_conv.double_conv.1.conv.0.bias
 (256)" fillcolor=lightblue]
	2146687766176 -> 2146688850288
	2146688850288 [label=AccumulateGrad]
	2146688850720 -> 2146688850768
	2146687766256 [label="down2.double_conv.double_conv.1.conv.1.weight
 (256)" fillcolor=lightblue]
	2146687766256 -> 2146688850720
	2146688850720 [label=AccumulateGrad]
	2146688851296 -> 2146688850768
	2146687766336 [label="down2.double_conv.double_conv.1.conv.1.bias
 (256)" fillcolor=lightblue]
	2146687766336 -> 2146688851296
	2146688851296 [label=AccumulateGrad]
	2146688851536 -> 2146688614352
	2146688851536 [label=ConstantPadNdBackward0]
	2146688850576 -> 2146688851536
	2146688850576 [label=UpsampleBilinear2DBackward1]
	2146688850048 -> 2146688850576
	2146688850048 [label=ReluBackward0]
	2146688848224 -> 2146688850048
	2146688848224 [label=CudnnBatchNormBackward0]
	2146688851392 -> 2146688848224
	2146688851392 [label=ConvolutionBackward0]
	2146705811824 -> 2146688851392
	2146705811824 [label=MulBackward0]
	2146705811440 -> 2146705811824
	2146705811440 [label=MulBackward0]
	2146705811200 -> 2146705811440
	2146705811200 [label=AddBackward0]
	2146705811008 -> 2146705811200
	2146705811008 [label=AddBackward0]
	2146705810768 -> 2146705811008
	2146705810768 [label=MulBackward0]
	2146705810432 -> 2146705810768
	2146705810432 [label=ReluBackward0]
	2146705810288 -> 2146705810432
	2146705810288 [label=CudnnBatchNormBackward0]
	2146705810096 -> 2146705810288
	2146705810096 [label=ConvolutionBackward0]
	2146705809712 -> 2146705810096
	2146705809712 [label=ReluBackward0]
	2146705812304 -> 2146705809712
	2146705812304 [label=CudnnBatchNormBackward0]
	2146705812400 -> 2146705812304
	2146705812400 [label=ConvolutionBackward0]
	2146705812592 -> 2146705812400
	2146705812592 [label=CatBackward0]
	2146705812784 -> 2146705812592
	2146705812784 [label=MaxPool2DWithIndicesBackward0]
	2146705812928 -> 2146705812784
	2146705812928 [label=ReluBackward0]
	2146705813024 -> 2146705812928
	2146705813024 [label=CudnnBatchNormBackward0]
	2146705813120 -> 2146705813024
	2146705813120 [label=ConvolutionBackward0]
	2146705813312 -> 2146705813120
	2146705813312 [label=ReluBackward0]
	2146705813456 -> 2146705813312
	2146705813456 [label=CudnnBatchNormBackward0]
	2145338327200 -> 2146705813456
	2145338327200 [label=ConvolutionBackward0]
	2146688851632 -> 2145338327200
	2145338327392 -> 2145338327200
	2146688155952 [label="down3.double_conv.double_conv.0.conv.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2146688155952 -> 2145338327392
	2145338327392 [label=AccumulateGrad]
	2145338327344 -> 2145338327200
	2146688156032 [label="down3.double_conv.double_conv.0.conv.0.bias
 (512)" fillcolor=lightblue]
	2146688156032 -> 2145338327344
	2145338327344 [label=AccumulateGrad]
	2145338327152 -> 2146705813456
	2146688156112 [label="down3.double_conv.double_conv.0.conv.1.weight
 (512)" fillcolor=lightblue]
	2146688156112 -> 2145338327152
	2145338327152 [label=AccumulateGrad]
	2145338327104 -> 2146705813456
	2146688156192 [label="down3.double_conv.double_conv.0.conv.1.bias
 (512)" fillcolor=lightblue]
	2146688156192 -> 2145338327104
	2145338327104 [label=AccumulateGrad]
	2146705813264 -> 2146705813120
	2146688156592 [label="down3.double_conv.double_conv.1.conv.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2146688156592 -> 2146705813264
	2146705813264 [label=AccumulateGrad]
	2146705813216 -> 2146705813120
	2146688156672 [label="down3.double_conv.double_conv.1.conv.0.bias
 (512)" fillcolor=lightblue]
	2146688156672 -> 2146705813216
	2146705813216 [label=AccumulateGrad]
	2146705813072 -> 2146705813024
	2146688156752 [label="down3.double_conv.double_conv.1.conv.1.weight
 (512)" fillcolor=lightblue]
	2146688156752 -> 2146705813072
	2146705813072 [label=AccumulateGrad]
	2146705812832 -> 2146705813024
	2146688156832 [label="down3.double_conv.double_conv.1.conv.1.bias
 (512)" fillcolor=lightblue]
	2146688156832 -> 2146705812832
	2146705812832 [label=AccumulateGrad]
	2146705812736 -> 2146705812592
	2146705812736 [label=ConstantPadNdBackward0]
	2146705813168 -> 2146705812736
	2146705813168 [label=UpsampleBilinear2DBackward1]
	2146705813408 -> 2146705813168
	2146705813408 [label=ReluBackward0]
	2146705812880 -> 2146705813408
	2146705812880 [label=CudnnBatchNormBackward0]
	2145338327488 -> 2146705812880
	2145338327488 [label=ConvolutionBackward0]
	2145338327680 -> 2145338327488
	2145338327680 [label=MulBackward0]
	2145338327872 -> 2145338327680
	2145338327872 [label=MaxPool2DWithIndicesBackward0]
	2145338328016 -> 2145338327872
	2145338328016 [label=ReluBackward0]
	2145338328112 -> 2145338328016
	2145338328112 [label=CudnnBatchNormBackward0]
	2145338328208 -> 2145338328112
	2145338328208 [label=ConvolutionBackward0]
	2145338328400 -> 2145338328208
	2145338328400 [label=ReluBackward0]
	2145338328592 -> 2145338328400
	2145338328592 [label=CudnnBatchNormBackward0]
	2145338328688 -> 2145338328592
	2145338328688 [label=ConvolutionBackward0]
	2146705812784 -> 2145338328688
	2145338328880 -> 2145338328688
	2146688157232 [label="down4.double_conv.double_conv.0.conv.0.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	2146688157232 -> 2145338328880
	2145338328880 [label=AccumulateGrad]
	2145338328832 -> 2145338328688
	2146688157312 [label="down4.double_conv.double_conv.0.conv.0.bias
 (1024)" fillcolor=lightblue]
	2146688157312 -> 2145338328832
	2145338328832 [label=AccumulateGrad]
	2145338328640 -> 2145338328592
	2146688157392 [label="down4.double_conv.double_conv.0.conv.1.weight
 (1024)" fillcolor=lightblue]
	2146688157392 -> 2145338328640
	2145338328640 [label=AccumulateGrad]
	2145338328496 -> 2145338328592
	2146688157472 [label="down4.double_conv.double_conv.0.conv.1.bias
 (1024)" fillcolor=lightblue]
	2146688157472 -> 2145338328496
	2145338328496 [label=AccumulateGrad]
	2145338328352 -> 2145338328208
	2146688157872 [label="down4.double_conv.double_conv.1.conv.0.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	2146688157872 -> 2145338328352
	2145338328352 [label=AccumulateGrad]
	2145338328304 -> 2145338328208
	2146688157952 [label="down4.double_conv.double_conv.1.conv.0.bias
 (1024)" fillcolor=lightblue]
	2146688157952 -> 2145338328304
	2145338328304 [label=AccumulateGrad]
	2145338328160 -> 2145338328112
	2146688158032 [label="down4.double_conv.double_conv.1.conv.1.weight
 (1024)" fillcolor=lightblue]
	2146688158032 -> 2145338328160
	2145338328160 [label=AccumulateGrad]
	2145338327920 -> 2145338328112
	2146688158112 [label="down4.double_conv.double_conv.1.conv.1.bias
 (1024)" fillcolor=lightblue]
	2146688158112 -> 2145338327920
	2145338327920 [label=AccumulateGrad]
	2145338327824 -> 2145338327680
	2145338327824 [label=ExpandBackward0]
	2145338328256 -> 2145338327824
	2145338328256 [label=ViewBackward0]
	2145338328928 -> 2145338328256
	2145338328928 [label=SigmoidBackward0]
	2145338328784 -> 2145338328928
	2145338328784 [label=MmBackward0]
	2145338329024 -> 2145338328784
	2145338329024 [label=ReluBackward0]
	2145338329168 -> 2145338329024
	2145338329168 [label=MmBackward0]
	2145338329264 -> 2145338329168
	2145338329264 [label=ViewBackward0]
	2145338329408 -> 2145338329264
	2145338329408 [label=MeanBackward1]
	2145338327872 -> 2145338329408
	2145338329216 -> 2145338329168
	2145338329216 [label=TBackward0]
	2145338329456 -> 2145338329216
	2146688158352 [label="se1.fc.0.weight
 (64, 1024)" fillcolor=lightblue]
	2146688158352 -> 2145338329456
	2145338329456 [label=AccumulateGrad]
	2145338328976 -> 2145338328784
	2145338328976 [label=TBackward0]
	2145338329504 -> 2145338328976
	2146688158432 [label="se1.fc.2.weight
 (1024, 64)" fillcolor=lightblue]
	2146688158432 -> 2145338329504
	2145338329504 [label=AccumulateGrad]
	2145338327632 -> 2145338327488
	2146688158512 [label="up1.reduce.conv.0.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2146688158512 -> 2145338327632
	2145338327632 [label=AccumulateGrad]
	2145338327584 -> 2145338327488
	2146688158592 [label="up1.reduce.conv.0.bias
 (512)" fillcolor=lightblue]
	2146688158592 -> 2145338327584
	2145338327584 [label=AccumulateGrad]
	2145338327440 -> 2146705812880
	2146688158672 [label="up1.reduce.conv.1.weight
 (512)" fillcolor=lightblue]
	2146688158672 -> 2145338327440
	2145338327440 [label=AccumulateGrad]
	2145338327248 -> 2146705812880
	2146688158752 [label="up1.reduce.conv.1.bias
 (512)" fillcolor=lightblue]
	2146688158752 -> 2145338327248
	2145338327248 [label=AccumulateGrad]
	2146705812544 -> 2146705812400
	2146688159152 [label="up1.conv.double_conv.0.conv.0.weight
 (512, 1024, 3, 3)" fillcolor=lightblue]
	2146688159152 -> 2146705812544
	2146705812544 [label=AccumulateGrad]
	2146705812496 -> 2146705812400
	2146688159232 [label="up1.conv.double_conv.0.conv.0.bias
 (512)" fillcolor=lightblue]
	2146688159232 -> 2146705812496
	2146705812496 [label=AccumulateGrad]
	2146705812352 -> 2146705812304
	2146688159312 [label="up1.conv.double_conv.0.conv.1.weight
 (512)" fillcolor=lightblue]
	2146688159312 -> 2146705812352
	2146705812352 [label=AccumulateGrad]
	2146705809520 -> 2146705812304
	2146688159392 [label="up1.conv.double_conv.0.conv.1.bias
 (512)" fillcolor=lightblue]
	2146688159392 -> 2146705809520
	2146705809520 [label=AccumulateGrad]
	2146705809760 -> 2146705810096
	2146688344208 [label="up1.conv.double_conv.1.conv.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2146688344208 -> 2146705809760
	2146705809760 [label=AccumulateGrad]
	2146705809904 -> 2146705810096
	2146688344288 [label="up1.conv.double_conv.1.conv.0.bias
 (512)" fillcolor=lightblue]
	2146688344288 -> 2146705809904
	2146705809904 [label=AccumulateGrad]
	2146705810144 -> 2146705810288
	2146688344368 [label="up1.conv.double_conv.1.conv.1.weight
 (512)" fillcolor=lightblue]
	2146688344368 -> 2146705810144
	2146705810144 [label=AccumulateGrad]
	2146705810384 -> 2146705810288
	2146688344448 [label="up1.conv.double_conv.1.conv.1.bias
 (512)" fillcolor=lightblue]
	2146688344448 -> 2146705810384
	2146705810384 [label=AccumulateGrad]
	2146705810576 -> 2146705810768
	2146705810576 [label=SigmoidBackward0]
	2146705809568 -> 2146705810576
	2146705809568 [label=CudnnBatchNormBackward0]
	2146705812448 -> 2146705809568
	2146705812448 [label=ConvolutionBackward0]
	2146705813360 -> 2146705812448
	2146705813360 [label=CatBackward0]
	2145338327728 -> 2146705813360
	2145338327728 [label=UnsqueezeBackward0]
	2145338328448 -> 2145338327728
	2145338328448 [label=MaxBackward0]
	2146705810432 -> 2145338328448
	2145338327536 -> 2146705813360
	2145338327536 [label=UnsqueezeBackward0]
	2145338328544 -> 2145338327536
	2145338328544 [label=MeanBackward1]
	2146705810432 -> 2145338328544
	2146705812976 -> 2146705812448
	2146688346048 [label="attention1.hw.conv.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	2146688346048 -> 2146705812976
	2146705812976 [label=AccumulateGrad]
	2146705809472 -> 2146705809568
	2146688345968 [label="attention1.hw.conv.bn.weight
 (1)" fillcolor=lightblue]
	2146688345968 -> 2146705809472
	2146705809472 [label=AccumulateGrad]
	2146705810336 -> 2146705809568
	2146688346128 [label="attention1.hw.conv.bn.bias
 (1)" fillcolor=lightblue]
	2146688346128 -> 2146705810336
	2146705810336 [label=AccumulateGrad]
	2146705810816 -> 2146705811008
	2146705810816 [label=CloneBackward0]
	2146705812640 -> 2146705810816
	2146705812640 [label=PermuteBackward0]
	2146705812688 -> 2146705812640
	2146705812688 [label=MulBackward0]
	2145338327296 -> 2146705812688
	2145338327296 [label=CloneBackward0]
	2145338329312 -> 2145338327296
	2145338329312 [label=PermuteBackward0]
	2146705810432 -> 2145338329312
	2145338327968 -> 2146705812688
	2145338327968 [label=SigmoidBackward0]
	2145338329552 -> 2145338327968
	2145338329552 [label=CudnnBatchNormBackward0]
	2145338329360 -> 2145338329552
	2145338329360 [label=ConvolutionBackward0]
	2145338329744 -> 2145338329360
	2145338329744 [label=CatBackward0]
	2145338329888 -> 2145338329744
	2145338329888 [label=UnsqueezeBackward0]
	2145338330032 -> 2145338329888
	2145338330032 [label=MaxBackward0]
	2145338327296 -> 2145338330032
	2145338329840 -> 2145338329744
	2145338329840 [label=UnsqueezeBackward0]
	2145338330080 -> 2145338329840
	2145338330080 [label=MeanBackward1]
	2145338327296 -> 2145338330080
	2145338329696 -> 2145338329360
	2146688344928 [label="attention1.cw.conv.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	2146688344928 -> 2145338329696
	2145338329696 [label=AccumulateGrad]
	2145338327776 -> 2145338329552
	2146688344848 [label="attention1.cw.conv.bn.weight
 (1)" fillcolor=lightblue]
	2146688344848 -> 2145338327776
	2145338327776 [label=AccumulateGrad]
	2145338328064 -> 2145338329552
	2146688345008 [label="attention1.cw.conv.bn.bias
 (1)" fillcolor=lightblue]
	2146688345008 -> 2145338328064
	2145338328064 [label=AccumulateGrad]
	2146705811152 -> 2146705811200
	2146705811152 [label=CloneBackward0]
	2146705810624 -> 2146705811152
	2146705810624 [label=PermuteBackward0]
	2146705810960 -> 2146705810624
	2146705810960 [label=MulBackward0]
	2145338329648 -> 2146705810960
	2145338329648 [label=CloneBackward0]
	2145338329792 -> 2145338329648
	2145338329792 [label=PermuteBackward0]
	2146705810432 -> 2145338329792
	2145338329600 -> 2146705810960
	2145338329600 [label=SigmoidBackward0]
	2145338329936 -> 2145338329600
	2145338329936 [label=CudnnBatchNormBackward0]
	2145338330224 -> 2145338329936
	2145338330224 [label=ConvolutionBackward0]
	2145338330416 -> 2145338330224
	2145338330416 [label=CatBackward0]
	2145338330560 -> 2145338330416
	2145338330560 [label=UnsqueezeBackward0]
	2145338330704 -> 2145338330560
	2145338330704 [label=MaxBackward0]
	2145338329648 -> 2145338330704
	2145338330512 -> 2145338330416
	2145338330512 [label=UnsqueezeBackward0]
	2145338330752 -> 2145338330512
	2145338330752 [label=MeanBackward1]
	2145338329648 -> 2145338330752
	2145338330368 -> 2145338330224
	2146688345488 [label="attention1.hc.conv.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	2146688345488 -> 2145338330368
	2145338330368 [label=AccumulateGrad]
	2145338330176 -> 2145338329936
	2146688345408 [label="attention1.hc.conv.bn.weight
 (1)" fillcolor=lightblue]
	2146688345408 -> 2145338330176
	2145338330176 [label=AccumulateGrad]
	2145338330128 -> 2145338329936
	2146688345568 [label="attention1.hc.conv.bn.bias
 (1)" fillcolor=lightblue]
	2146688345568 -> 2145338330128
	2145338330128 [label=AccumulateGrad]
	2146705811488 -> 2146705811824
	2146705811488 [label=ExpandBackward0]
	2146705809952 -> 2146705811488
	2146705809952 [label=ViewBackward0]
	2146705811248 -> 2146705809952
	2146705811248 [label=SigmoidBackward0]
	2145338330272 -> 2146705811248
	2145338330272 [label=MmBackward0]
	2145338330848 -> 2145338330272
	2145338330848 [label=ReluBackward0]
	2145338330608 -> 2145338330848
	2145338330608 [label=MmBackward0]
	2145338330896 -> 2145338330608
	2145338330896 [label=ViewBackward0]
	2145338331040 -> 2145338330896
	2145338331040 [label=MeanBackward1]
	2146705811440 -> 2145338331040
	2145338330656 -> 2145338330608
	2145338330656 [label=TBackward0]
	2145338330944 -> 2145338330656
	2146688346448 [label="se2.fc.0.weight
 (32, 512)" fillcolor=lightblue]
	2146688346448 -> 2145338330944
	2145338330944 [label=AccumulateGrad]
	2145338330320 -> 2145338330272
	2145338330320 [label=TBackward0]
	2145338331088 -> 2145338330320
	2146688346528 [label="se2.fc.2.weight
 (512, 32)" fillcolor=lightblue]
	2146688346528 -> 2145338331088
	2145338331088 [label=AccumulateGrad]
	2146705811872 -> 2146688851392
	2146688346688 [label="up2.reduce.conv.0.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2146688346688 -> 2146705811872
	2146705811872 [label=AccumulateGrad]
	2146705812064 -> 2146688851392
	2146688346768 [label="up2.reduce.conv.0.bias
 (256)" fillcolor=lightblue]
	2146688346768 -> 2146705812064
	2146705812064 [label=AccumulateGrad]
	2146705812160 -> 2146688848224
	2146688346848 [label="up2.reduce.conv.1.weight
 (256)" fillcolor=lightblue]
	2146688346848 -> 2146705812160
	2146705812160 [label=AccumulateGrad]
	2146705812112 -> 2146688848224
	2146688346928 [label="up2.reduce.conv.1.bias
 (256)" fillcolor=lightblue]
	2146688346928 -> 2146705812112
	2146705812112 [label=AccumulateGrad]
	2146688613824 -> 2146688613968
	2146688347328 [label="up2.conv.double_conv.0.conv.0.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	2146688347328 -> 2146688613824
	2146688613824 [label=AccumulateGrad]
	2146688613872 -> 2146688613968
	2146688347408 [label="up2.conv.double_conv.0.conv.0.bias
 (256)" fillcolor=lightblue]
	2146688347408 -> 2146688613872
	2146688613872 [label=AccumulateGrad]
	2146688613728 -> 2146688613680
	2146688347488 [label="up2.conv.double_conv.0.conv.1.weight
 (256)" fillcolor=lightblue]
	2146688347488 -> 2146688613728
	2146688613728 [label=AccumulateGrad]
	2146688613440 -> 2146688613680
	2146688347568 [label="up2.conv.double_conv.0.conv.1.bias
 (256)" fillcolor=lightblue]
	2146688347568 -> 2146688613440
	2146688613440 [label=AccumulateGrad]
	2146688876704 -> 2146688877568
	2146688347968 [label="up2.conv.double_conv.1.conv.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2146688347968 -> 2146688876704
	2146688876704 [label=AccumulateGrad]
	2146688877088 -> 2146688877568
	2146688348048 [label="up2.conv.double_conv.1.conv.0.bias
 (256)" fillcolor=lightblue]
	2146688348048 -> 2146688877088
	2146688877088 [label=AccumulateGrad]
	2146688877520 -> 2146688878192
	2146688495680 [label="up2.conv.double_conv.1.conv.1.weight
 (256)" fillcolor=lightblue]
	2146688495680 -> 2146688877520
	2146688877520 [label=AccumulateGrad]
	2146688878816 -> 2146688878192
	2146688495760 [label="up2.conv.double_conv.1.conv.1.bias
 (256)" fillcolor=lightblue]
	2146688495760 -> 2146688878816
	2146688878816 [label=AccumulateGrad]
	2146688879392 -> 2146705604720
	2146688879392 [label=SigmoidBackward0]
	2146688876992 -> 2146688879392
	2146688876992 [label=CudnnBatchNormBackward0]
	2146688613920 -> 2146688876992
	2146688613920 [label=ConvolutionBackward0]
	2146688850192 -> 2146688613920
	2146688850192 [label=CatBackward0]
	2146688849088 -> 2146688850192
	2146688849088 [label=UnsqueezeBackward0]
	2146705811296 -> 2146688849088
	2146705811296 [label=MaxBackward0]
	2146688878768 -> 2146705811296
	2146705811680 -> 2146688850192
	2146705811680 [label=UnsqueezeBackward0]
	2146705811632 -> 2146705811680
	2146705811632 [label=MeanBackward1]
	2146688878768 -> 2146705811632
	2146688850960 -> 2146688613920
	2146688497360 [label="attention2.hw.conv.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	2146688497360 -> 2146688850960
	2146688850960 [label=AccumulateGrad]
	2146688613488 -> 2146688876992
	2146688497280 [label="attention2.hw.conv.bn.weight
 (1)" fillcolor=lightblue]
	2146688497280 -> 2146688613488
	2146688613488 [label=AccumulateGrad]
	2146688613392 -> 2146688876992
	2146688497440 [label="attention2.hw.conv.bn.bias
 (1)" fillcolor=lightblue]
	2146688497440 -> 2146688613392
	2146688613392 [label=AccumulateGrad]
	2146688879968 -> 2146705604672
	2146688879968 [label=CloneBackward0]
	2146688878144 -> 2146688879968
	2146688878144 [label=PermuteBackward0]
	2146705812016 -> 2146688878144
	2146705812016 [label=MulBackward0]
	2146688851824 -> 2146705812016
	2146688851824 [label=CloneBackward0]
	2145338330992 -> 2146688851824
	2145338330992 [label=PermuteBackward0]
	2146688878768 -> 2145338330992
	2145338329984 -> 2146705812016
	2145338329984 [label=SigmoidBackward0]
	2145338329072 -> 2145338329984
	2145338329072 [label=CudnnBatchNormBackward0]
	2145338329120 -> 2145338329072
	2145338329120 [label=ConvolutionBackward0]
	2145338368304 -> 2145338329120
	2145338368304 [label=CatBackward0]
	2145338368448 -> 2145338368304
	2145338368448 [label=UnsqueezeBackward0]
	2145338368592 -> 2145338368448
	2145338368592 [label=MaxBackward0]
	2146688851824 -> 2145338368592
	2145338368400 -> 2145338368304
	2145338368400 [label=UnsqueezeBackward0]
	2145338368640 -> 2145338368400
	2145338368640 [label=MeanBackward1]
	2146688851824 -> 2145338368640
	2145338368256 -> 2145338329120
	2146688496240 [label="attention2.cw.conv.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	2146688496240 -> 2145338368256
	2145338368256 [label=AccumulateGrad]
	2145338368112 -> 2145338329072
	2146688496160 [label="attention2.cw.conv.bn.weight
 (1)" fillcolor=lightblue]
	2146688496160 -> 2145338368112
	2145338368112 [label=AccumulateGrad]
	2145338368064 -> 2145338329072
	2146688496320 [label="attention2.cw.conv.bn.bias
 (1)" fillcolor=lightblue]
	2146688496320 -> 2145338368064
	2145338368064 [label=AccumulateGrad]
	2146705605440 -> 2146705605392
	2146705605440 [label=CloneBackward0]
	2146688851920 -> 2146705605440
	2146688851920 [label=PermuteBackward0]
	2145338330464 -> 2146688851920
	2145338330464 [label=MulBackward0]
	2146688879920 -> 2145338330464
	2146688879920 [label=CloneBackward0]
	2145338368352 -> 2146688879920
	2145338368352 [label=PermuteBackward0]
	2146688878768 -> 2145338368352
	2145338368208 -> 2145338330464
	2145338368208 [label=SigmoidBackward0]
	2145338368496 -> 2145338368208
	2145338368496 [label=CudnnBatchNormBackward0]
	2145338368784 -> 2145338368496
	2145338368784 [label=ConvolutionBackward0]
	2145338368976 -> 2145338368784
	2145338368976 [label=CatBackward0]
	2145338369120 -> 2145338368976
	2145338369120 [label=UnsqueezeBackward0]
	2145338369264 -> 2145338369120
	2145338369264 [label=MaxBackward0]
	2146688879920 -> 2145338369264
	2145338369072 -> 2145338368976
	2145338369072 [label=UnsqueezeBackward0]
	2145338369312 -> 2145338369072
	2145338369312 [label=MeanBackward1]
	2146688879920 -> 2145338369312
	2145338368928 -> 2145338368784
	2146688496800 [label="attention2.hc.conv.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	2146688496800 -> 2145338368928
	2145338368928 [label=AccumulateGrad]
	2145338368736 -> 2145338368496
	2146688496720 [label="attention2.hc.conv.bn.weight
 (1)" fillcolor=lightblue]
	2146688496720 -> 2145338368736
	2145338368736 [label=AccumulateGrad]
	2145338368688 -> 2145338368496
	2146688496880 [label="attention2.hc.conv.bn.bias
 (1)" fillcolor=lightblue]
	2146688496880 -> 2145338368688
	2145338368688 [label=AccumulateGrad]
	2146705606592 -> 2146705607936
	2146705606592 [label=ExpandBackward0]
	2146688879344 -> 2146705606592
	2146688879344 [label=ViewBackward0]
	2145338330800 -> 2146688879344
	2145338330800 [label=SigmoidBackward0]
	2145338368832 -> 2145338330800
	2145338368832 [label=MmBackward0]
	2145338369408 -> 2145338368832
	2145338369408 [label=ReluBackward0]
	2145338369168 -> 2145338369408
	2145338369168 [label=MmBackward0]
	2145338369456 -> 2145338369168
	2145338369456 [label=ViewBackward0]
	2145338369600 -> 2145338369456
	2145338369600 [label=MeanBackward1]
	2146705606640 -> 2145338369600
	2145338369216 -> 2145338369168
	2145338369216 [label=TBackward0]
	2145338369648 -> 2145338369216
	2146688497760 [label="se3.fc.0.weight
 (16, 256)" fillcolor=lightblue]
	2146688497760 -> 2145338369648
	2145338369648 [label=AccumulateGrad]
	2145338368880 -> 2145338368832
	2145338368880 [label=TBackward0]
	2145338369696 -> 2145338368880
	2146688497840 [label="se3.fc.2.weight
 (256, 16)" fillcolor=lightblue]
	2146688497840 -> 2145338369696
	2145338369696 [label=AccumulateGrad]
	2146705607888 -> 2146705617008
	2146688498000 [label="up3.reduce.conv.0.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2146688498000 -> 2146705607888
	2146705607888 [label=AccumulateGrad]
	2146705608656 -> 2146705617008
	2146688498080 [label="up3.reduce.conv.0.bias
 (128)" fillcolor=lightblue]
	2146688498080 -> 2146705608656
	2146705608656 [label=AccumulateGrad]
	2146705617056 -> 2146705617392
	2146688498160 [label="up3.reduce.conv.1.weight
 (128)" fillcolor=lightblue]
	2146688498160 -> 2146705617056
	2146705617056 [label=AccumulateGrad]
	2146705618784 -> 2146705617392
	2146688498240 [label="up3.reduce.conv.1.bias
 (128)" fillcolor=lightblue]
	2146688498240 -> 2146705618784
	2146705618784 [label=AccumulateGrad]
	2146705619504 -> 2146705619648
	2146688498640 [label="up3.conv.double_conv.0.conv.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	2146688498640 -> 2146705619504
	2146705619504 [label=AccumulateGrad]
	2146705619552 -> 2146705619648
	2146688498720 [label="up3.conv.double_conv.0.conv.0.bias
 (128)" fillcolor=lightblue]
	2146688498720 -> 2146705619552
	2146705619552 [label=AccumulateGrad]
	2146705619792 -> 2146705619840
	2146688498800 [label="up3.conv.double_conv.0.conv.1.weight
 (128)" fillcolor=lightblue]
	2146688498800 -> 2146705619792
	2146705619792 [label=AccumulateGrad]
	2146705620032 -> 2146705619840
	2146688498880 [label="up3.conv.double_conv.0.conv.1.bias
 (128)" fillcolor=lightblue]
	2146688498880 -> 2146705620032
	2146705620032 [label=AccumulateGrad]
	2146705620368 -> 2146705620512
	2146688499280 [label="up3.conv.double_conv.1.conv.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2146688499280 -> 2146705620368
	2146705620368 [label=AccumulateGrad]
	2146705620416 -> 2146705620512
	2146688499360 [label="up3.conv.double_conv.1.conv.0.bias
 (128)" fillcolor=lightblue]
	2146688499360 -> 2146705620416
	2146705620416 [label=AccumulateGrad]
	2146705620656 -> 2146705620704
	2146688499440 [label="up3.conv.double_conv.1.conv.1.weight
 (128)" fillcolor=lightblue]
	2146688499440 -> 2146705620656
	2146705620656 [label=AccumulateGrad]
	2146705620896 -> 2146705620704
	2146688499520 [label="up3.conv.double_conv.1.conv.1.bias
 (128)" fillcolor=lightblue]
	2146688499520 -> 2146705620896
	2146705620896 [label=AccumulateGrad]
	2146705641920 -> 2146705642064
	2146705641920 [label=TBackward0]
	2146705641824 -> 2146705641920
	2146688655504 [label="attention3.mlp1.weight
 (384, 128)" fillcolor=lightblue]
	2146688655504 -> 2146705641824
	2146705641824 [label=AccumulateGrad]
	2146705642448 -> 2146705642640
	2146688655584 [label="attention3.mlp1.bias
 (384)" fillcolor=lightblue]
	2146688655584 -> 2146705642448
	2146705642448 [label=AccumulateGrad]
	2146705642688 -> 2146705642784
	2146705642688 [label=SliceBackward0]
	2146705642256 -> 2146705642688
	2146705642256 [label=SliceBackward0]
	2146705642112 -> 2146705642256
	2146705642112 [label=SliceBackward0]
	2146705620848 -> 2146705642112
	2146705620848 [label=SliceBackward0]
	2146705619600 -> 2146705620848
	2146705619600 [label=SliceBackward0]
	2146705619168 -> 2146705619600
	2146705619168 [label=SliceBackward0]
	2146705617920 -> 2146705619168
	2146705617920 [label=SliceBackward0]
	2146705617440 -> 2146705617920
	2146705617440 [label=SliceBackward0]
	2146705642640 -> 2146705617440
	2146705642928 -> 2146705643120
	2146705642928 [label=SliceBackward0]
	2146705642496 -> 2146705642928
	2146705642496 [label=SliceBackward0]
	2146705642736 -> 2146705642496
	2146705642736 [label=SliceBackward0]
	2146705618688 -> 2146705642736
	2146705618688 [label=SliceBackward0]
	2146705620176 -> 2146705618688
	2146705620176 [label=AsStridedBackward0]
	2146705642784 -> 2146705620176
	2146705643168 -> 2146705643360
	2146705643168 [label=SliceBackward0]
	2146705641776 -> 2146705643168
	2146705641776 [label=SliceBackward0]
	2146705620464 -> 2146705641776
	2146705620464 [label=SliceBackward0]
	2146705619984 -> 2146705620464
	2146705619984 [label=SliceBackward0]
	2146705606016 -> 2146705619984
	2146705606016 [label=AsStridedBackward0]
	2146705643120 -> 2146705606016
	2146705643504 -> 2146705643600
	2146705643504 [label=SliceBackward0]
	2146705619312 -> 2146705643504
	2146705619312 [label=SliceBackward0]
	2146705643312 -> 2146705619312
	2146705643312 [label=SliceBackward0]
	2146705608608 -> 2146705643312
	2146705608608 [label=SliceBackward0]
	2146705607168 -> 2146705608608
	2146705607168 [label=AsStridedBackward0]
	2146705643360 -> 2146705607168
	2146705643648 -> 2146705643840
	2146705643648 [label=SliceBackward0]
	2146705642976 -> 2146705643648
	2146705642976 [label=SliceBackward0]
	2146705606064 -> 2146705642976
	2146705606064 [label=SliceBackward0]
	2145338368544 -> 2146705606064
	2145338368544 [label=SliceBackward0]
	2145338369024 -> 2145338368544
	2145338369024 [label=SliceBackward0]
	2145338369792 -> 2145338369024
	2145338369792 [label=SliceBackward0]
	2145338369888 -> 2145338369792
	2145338369888 [label=SliceBackward0]
	2145338369984 -> 2145338369888
	2145338369984 [label=SliceBackward0]
	2146705643600 -> 2145338369984
	2146705643984 -> 2146705644176
	2146705643984 [label=SliceBackward0]
	2146705607216 -> 2146705643984
	2146705607216 [label=SliceBackward0]
	2146705643792 -> 2146705607216
	2146705643792 [label=SliceBackward0]
	2145338369840 -> 2146705643792
	2145338369840 [label=SliceBackward0]
	2145338370080 -> 2145338369840
	2145338370080 [label=AsStridedBackward0]
	2146705643840 -> 2145338370080
	2146705644224 -> 2146705644416
	2146705644224 [label=SliceBackward0]
	2146705643552 -> 2146705644224
	2146705643552 [label=SliceBackward0]
	2145338369936 -> 2146705643552
	2145338369936 [label=SliceBackward0]
	2145338370032 -> 2145338369936
	2145338370032 [label=SliceBackward0]
	2145338370128 -> 2145338370032
	2145338370128 [label=AsStridedBackward0]
	2146705644176 -> 2145338370128
	2146705644464 -> 2146705644512
	2146705644464 [label=SliceBackward0]
	2146705644032 -> 2146705644464
	2146705644032 [label=SliceBackward0]
	2145338369504 -> 2146705644032
	2145338369504 [label=SliceBackward0]
	2145338370176 -> 2145338369504
	2145338370176 [label=SliceBackward0]
	2145338370272 -> 2145338370176
	2145338370272 [label=AsStridedBackward0]
	2146705644416 -> 2145338370272
	2146705645040 -> 2146705645088
	2146705645040 [label=AsStridedBackward0]
	2146705644512 -> 2146705645040
	2146705645280 -> 2146705645088
	2146705645280 [label=SliceBackward0]
	2146705644656 -> 2146705645280
	2146705644656 [label=SliceBackward0]
	2146705644368 -> 2146705644656
	2146705644368 [label=SliceBackward0]
	2145338370368 -> 2146705644368
	2145338370368 [label=SliceBackward0]
	2146705644512 -> 2145338370368
	2146705674448 -> 2146705674496
	2146705674448 [label=TBackward0]
	2146705645520 -> 2146705674448
	2146688655744 [label="attention3.split_attention.mlp1.weight
 (128, 128)" fillcolor=lightblue]
	2146688655744 -> 2146705645520
	2146705645520 [label=AccumulateGrad]
	2146705674880 -> 2146705674928
	2146705674880 [label=TBackward0]
	2146705674688 -> 2146705674880
	2146688655824 [label="attention3.split_attention.mlp2.weight
 (384, 128)" fillcolor=lightblue]
	2146688655824 -> 2146705674688
	2146705674688 [label=AccumulateGrad]
	2146705675696 -> 2146705675744
	2146705676416 -> 2146705676560
	2146705676416 [label=TBackward0]
	2146705675792 -> 2146705676416
	2146682802560 [label="attention3.mlp2.weight
 (128, 128)" fillcolor=lightblue]
	2146682802560 -> 2146705675792
	2146705675792 [label=AccumulateGrad]
	2146705676848 -> 2146705676896
	2146688655664 [label="attention3.mlp2.bias
 (128)" fillcolor=lightblue]
	2146688655664 -> 2146705676848
	2146705676848 [label=AccumulateGrad]
	2146705677280 -> 2146705677520
	2146705677280 [label=ExpandBackward0]
	2146705675984 -> 2146705677280
	2146705675984 [label=ViewBackward0]
	2146705676608 -> 2146705675984
	2146705676608 [label=SigmoidBackward0]
	2146705676224 -> 2146705676608
	2146705676224 [label=MmBackward0]
	2146705675168 -> 2146705676224
	2146705675168 [label=ReluBackward0]
	2146705674640 -> 2146705675168
	2146705674640 [label=MmBackward0]
	2146705674976 -> 2146705674640
	2146705674976 [label=ViewBackward0]
	2146705645376 -> 2146705674976
	2146705645376 [label=AsStridedBackward1]
	2145338369552 -> 2146705645376
	2145338369552 [label=MeanBackward1]
	2146705677232 -> 2145338369552
	2146705644704 -> 2146705674640
	2146705644704 [label=TBackward0]
	2146705645232 -> 2146705644704
	2146684507296 [label="se4.fc.0.weight
 (8, 128)" fillcolor=lightblue]
	2146684507296 -> 2146705645232
	2146705645232 [label=AccumulateGrad]
	2146705676176 -> 2146705676224
	2146705676176 [label=TBackward0]
	2146705644848 -> 2146705676176
	2146688655904 [label="se4.fc.2.weight
 (128, 8)" fillcolor=lightblue]
	2146688655904 -> 2146705644848
	2146705644848 [label=AccumulateGrad]
	2146705677568 -> 2146705677904
	2146688656064 [label="up4.reduce.conv.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	2146688656064 -> 2146705677568
	2146705677568 [label=AccumulateGrad]
	2146705677712 -> 2146705677904
	2146688656144 [label="up4.reduce.conv.0.bias
 (64)" fillcolor=lightblue]
	2146688656144 -> 2146705677712
	2146705677712 [label=AccumulateGrad]
	2146705677952 -> 2146705716560
	2146688656224 [label="up4.reduce.conv.1.weight
 (64)" fillcolor=lightblue]
	2146688656224 -> 2146705677952
	2146705677952 [label=AccumulateGrad]
	2146705678288 -> 2146705716560
	2146688656304 [label="up4.reduce.conv.1.bias
 (64)" fillcolor=lightblue]
	2146688656304 -> 2146705678288
	2146705678288 [label=AccumulateGrad]
	2146705717088 -> 2146705717424
	2146688656704 [label="up4.conv.double_conv.0.conv.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2146688656704 -> 2146705717088
	2146705717088 [label=AccumulateGrad]
	2146705717232 -> 2146705717424
	2146688656784 [label="up4.conv.double_conv.0.conv.0.bias
 (64)" fillcolor=lightblue]
	2146688656784 -> 2146705717232
	2146705717232 [label=AccumulateGrad]
	2146705717472 -> 2146705717616
	2146688656864 [label="up4.conv.double_conv.0.conv.1.weight
 (64)" fillcolor=lightblue]
	2146688656864 -> 2146705717472
	2146705717472 [label=AccumulateGrad]
	2146705717808 -> 2146705717616
	2146688656944 [label="up4.conv.double_conv.0.conv.1.bias
 (64)" fillcolor=lightblue]
	2146688656944 -> 2146705717808
	2146705717808 [label=AccumulateGrad]
	2146705717952 -> 2146705718288
	2146688657344 [label="up4.conv.double_conv.1.conv.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2146688657344 -> 2146705717952
	2146705717952 [label=AccumulateGrad]
	2146705718096 -> 2146705718288
	2146688657424 [label="up4.conv.double_conv.1.conv.0.bias
 (64)" fillcolor=lightblue]
	2146688657424 -> 2146705718096
	2146705718096 [label=AccumulateGrad]
	2146705718336 -> 2146705718480
	2146688657504 [label="up4.conv.double_conv.1.conv.1.weight
 (64)" fillcolor=lightblue]
	2146688657504 -> 2146705718336
	2146705718336 [label=AccumulateGrad]
	2146705718960 -> 2146705718480
	2146688657584 [label="up4.conv.double_conv.1.conv.1.bias
 (64)" fillcolor=lightblue]
	2146688657584 -> 2146705718960
	2146705718960 [label=AccumulateGrad]
	2146705719152 -> 2146705743984
	2146705719152 [label=TBackward0]
	2146705718528 -> 2146705719152
	2146688657904 [label="attention4.mlp1.weight
 (192, 64)" fillcolor=lightblue]
	2146688657904 -> 2146705718528
	2146705718528 [label=AccumulateGrad]
	2146705744224 -> 2146705744320
	2146688657984 [label="attention4.mlp1.bias
 (192)" fillcolor=lightblue]
	2146688657984 -> 2146705744224
	2146705744224 [label=AccumulateGrad]
	2146705744464 -> 2146705744656
	2146705744464 [label=SliceBackward0]
	2146705744032 -> 2146705744464
	2146705744032 [label=SliceBackward0]
	2146705717856 -> 2146705744032
	2146705717856 [label=SliceBackward0]
	2146705718816 -> 2146705717856
	2146705718816 [label=SliceBackward0]
	2146705717280 -> 2146705718816
	2146705717280 [label=SliceBackward0]
	2146705716944 -> 2146705717280
	2146705716944 [label=SliceBackward0]
	2146705715696 -> 2146705716944
	2146705715696 [label=SliceBackward0]
	2146705718720 -> 2146705715696
	2146705718720 [label=SliceBackward0]
	2146705744320 -> 2146705718720
	2146705744704 -> 2146705744896
	2146705744704 [label=SliceBackward0]
	2146705744272 -> 2146705744704
	2146705744272 [label=SliceBackward0]
	2146705717664 -> 2146705744272
	2146705717664 [label=SliceBackward0]
	2146705716176 -> 2146705717664
	2146705716176 [label=SliceBackward0]
	2146705719200 -> 2146705716176
	2146705719200 [label=AsStridedBackward0]
	2146705744656 -> 2146705719200
	2146705745040 -> 2146705745136
	2146705745040 [label=SliceBackward0]
	2146705744512 -> 2146705745040
	2146705744512 [label=SliceBackward0]
	2146705716992 -> 2146705744512
	2146705716992 [label=SliceBackward0]
	2146705677472 -> 2146705716992
	2146705677472 [label=SliceBackward0]
	2146705677088 -> 2146705677472
	2146705677088 [label=AsStridedBackward0]
	2146705744896 -> 2146705677088
	2146705745184 -> 2146705745376
	2146705745184 [label=SliceBackward0]
	2146705718144 -> 2146705745184
	2146705718144 [label=SliceBackward0]
	2146705745088 -> 2146705718144
	2146705745088 [label=SliceBackward0]
	2146705676656 -> 2146705745088
	2146705676656 [label=SliceBackward0]
	2146705677040 -> 2146705676656
	2146705677040 [label=AsStridedBackward0]
	2146705745136 -> 2146705677040
	2146705745520 -> 2146705745712
	2146705745520 [label=SliceBackward0]
	2146705744848 -> 2146705745520
	2146705744848 [label=SliceBackward0]
	2146705677424 -> 2146705744848
	2146705677424 [label=SliceBackward0]
	2146705678096 -> 2146705677424
	2146705678096 [label=SliceBackward0]
	2146705677760 -> 2146705678096
	2146705677760 [label=SliceBackward0]
	2145338369744 -> 2146705677760
	2145338369744 [label=SliceBackward0]
	2145338370464 -> 2145338369744
	2145338370464 [label=SliceBackward0]
	2145338370560 -> 2145338370464
	2145338370560 [label=SliceBackward0]
	2146705745376 -> 2145338370560
	2146705745760 -> 2146705745952
	2146705745760 [label=SliceBackward0]
	2146705745328 -> 2146705745760
	2146705745328 [label=SliceBackward0]
	2146705675504 -> 2146705745328
	2146705675504 [label=SliceBackward0]
	2145338370416 -> 2146705675504
	2145338370416 [label=SliceBackward0]
	2145338370656 -> 2145338370416
	2145338370656 [label=AsStridedBackward0]
	2146705745712 -> 2145338370656
	2146705746000 -> 2146705746192
	2146705746000 [label=SliceBackward0]
	2146705675360 -> 2146705746000
	2146705675360 [label=SliceBackward0]
	2146705745904 -> 2146705675360
	2146705745904 [label=SliceBackward0]
	2145338370608 -> 2146705745904
	2145338370608 [label=SliceBackward0]
	2145338370704 -> 2145338370608
	2145338370704 [label=AsStridedBackward0]
	2146705745952 -> 2145338370704
	2146705746240 -> 2146705746384
	2146705746240 [label=SliceBackward0]
	2146705745568 -> 2146705746240
	2146705745568 [label=SliceBackward0]
	2145338370224 -> 2146705745568
	2145338370224 [label=SliceBackward0]
	2145338370752 -> 2145338370224
	2145338370752 [label=SliceBackward0]
	2145338370848 -> 2145338370752
	2145338370848 [label=AsStridedBackward0]
	2146705746192 -> 2145338370848
	2146705746816 -> 2146705746864
	2146705746816 [label=AsStridedBackward0]
	2146705746384 -> 2146705746816
	2146705747056 -> 2146705746864
	2146705747056 [label=SliceBackward0]
	2146705746432 -> 2146705747056
	2146705746432 [label=SliceBackward0]
	2146705746048 -> 2146705746432
	2146705746048 [label=SliceBackward0]
	2145338370944 -> 2146705746048
	2145338370944 [label=SliceBackward0]
	2146705746384 -> 2145338370944
	2146705747488 -> 2146705747632
	2146705747488 [label=TBackward0]
	2146705747296 -> 2146705747488
	2146688658224 [label="attention4.split_attention.mlp1.weight
 (64, 64)" fillcolor=lightblue]
	2146688658224 -> 2146705747296
	2146705747296 [label=AccumulateGrad]
	2146705747776 -> 2146705776704
	2146705747776 [label=TBackward0]
	2146705746912 -> 2146705747776
	2146688658304 [label="attention4.split_attention.mlp2.weight
 (192, 64)" fillcolor=lightblue]
	2146688658304 -> 2146705746912
	2146705746912 [label=AccumulateGrad]
	2146705777376 -> 2146705777520
	2146705778192 -> 2146705778240
	2146705778192 [label=TBackward0]
	2146705777568 -> 2146705778192
	2146688658064 [label="attention4.mlp2.weight
 (64, 64)" fillcolor=lightblue]
	2146688658064 -> 2146705777568
	2146705777568 [label=AccumulateGrad]
	2146705778624 -> 2146705778768
	2146688658144 [label="attention4.mlp2.bias
 (64)" fillcolor=lightblue]
	2146688658144 -> 2146705778624
	2146705778624 [label=AccumulateGrad]
	2146705779920 -> 2146705779056
	2146705779920 [label=ExpandBackward0]
	2146705777760 -> 2146705779920
	2146705777760 [label=ViewBackward0]
	2146705778384 -> 2146705777760
	2146705778384 [label=SigmoidBackward0]
	2146705778096 -> 2146705778384
	2146705778096 [label=MmBackward0]
	2146705777040 -> 2146705778096
	2146705777040 [label=ReluBackward0]
	2146705776848 -> 2146705777040
	2146705776848 [label=MmBackward0]
	2146705746576 -> 2146705776848
	2146705746576 [label=ViewBackward0]
	2146705747248 -> 2146705746576
	2146705747248 [label=AsStridedBackward1]
	2145338370512 -> 2146705747248
	2145338370512 [label=MeanBackward1]
	2146705779872 -> 2145338370512
	2146705746624 -> 2146705776848
	2146705746624 [label=TBackward0]
	2146705747680 -> 2146705746624
	2146688658384 [label="se5.fc.0.weight
 (4, 64)" fillcolor=lightblue]
	2146688658384 -> 2146705747680
	2146705747680 [label=AccumulateGrad]
	2146705777952 -> 2146705778096
	2146705777952 [label=TBackward0]
	2146705747920 -> 2146705777952
	2146688658464 [label="se5.fc.2.weight
 (64, 4)" fillcolor=lightblue]
	2146688658464 -> 2146705747920
	2146705747920 [label=AccumulateGrad]
	2146705779632 -> 2146705779440
	2146688658624 [label="outconv.weight
 (3, 64, 1, 1)" fillcolor=lightblue]
	2146688658624 -> 2146705779632
	2146705779632 [label=AccumulateGrad]
	2146705779104 -> 2146705779440
	2146688658704 [label="outconv.bias
 (3)" fillcolor=lightblue]
	2146688658704 -> 2146705779104
	2146705779104 [label=AccumulateGrad]
	2146705779440 -> 2146705655344
}
